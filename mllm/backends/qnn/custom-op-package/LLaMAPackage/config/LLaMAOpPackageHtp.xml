<?xml version="1.0" encoding="UTF-8"?>
<!--
Copyright (c) 2020 Qualcomm Technologies, Inc.
All Rights Reserved.
Confidential and Proprietary - Qualcomm Technologies, Inc.
-->
<OpDefCollection
        PackageName="LLaMAPackage"
        Domain="LLaMA"
        Version="1.0"
>
    <OpDefList>
        <!--Example Op Package which shows how a package can be defined using supplemental info-->
        <OpDef>
            <Name>LLaMASuperSiLU</Name>
            <Description>
                <Content>
                    fused SiLU function
                </Content>
            </Description>

            <Reference Source="Torch"
                       Url="https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"/>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>a_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <Parameter>
                <Name>b_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <Parameter>
                <Name>o_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>SiLU</Name>
            <Description>
                <Content>
                    Applies the Sigmoid Linear Unit (SiLU) function, element-wise. The SiLU function is also known as the swish function.
                </Content>
            </Description>

            <Reference Source="Torch"
                       Url="https://pytorch.org/docs/stable/generated/torch.nn.SiLU.html"/>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMAReLU</Name>
            <Description>
                <Content>
                    LLaMA ReLU
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMALinear</Name>
            <Description>
                <Content>
                    LLaMA Linear
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[2]</Name>
                <Description>
                    <Content>bias</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>in_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <Parameter>
                <Name>weight_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <Parameter>
                <Name>bias_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <Parameter>
                <Name>output_scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>QLayerNorm</Name>
            <Description>
                <Content>
                    LayerNorm QFP version
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>weights</Name>
                <Description>
                    <Content>LayerNorm weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[ EMB]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>bias</Name>
                <Description>
                    <Content>LayerNorm weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[ EMB]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>RMSNorm</Name>
            <Description>
                <Content>
                    LLaMA RMSNorm
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>weights</Name>
                <Description>
                    <Content>RMSNorm weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[ EMB]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>RoPE</Name>
            <Description>
                <Content>
                    LLaMA RoPE
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>sin</Name>
                <Description>
                    <Content>RoPE sin weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ 16384, hidden state ]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>cos</Name>
                <Description>
                    <Content>RoPE cos weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ 16384, hidden state  ]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>h_cnt</Name>
                <Description>
                    <Content>h_cnt</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Input>


            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>pose_type</Name>
                <Mandatory>false</Mandatory>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>IRoPE</Name>
            <Description>
                <Content>
                    LLaMA IRoPE
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>sin</Name>
                <Description>
                    <Content>RoPE sin weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ 16384, hidden state ]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>cos</Name>
                <Description>
                    <Content>RoPE cos weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ 16384, hidden state  ]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>h_cnt</Name>
                <Description>
                    <Content>h_cnt</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Input>


            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>pose_type</Name>
                <Mandatory>false</Mandatory>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>RoPESimple</Name>
            <Description>
                <Content>
                    LLaMA RoPE Simple
                    Only calculate:
                    value1 = in_value * cos_value - in_value_2 * sin_value and 
                    value2 = in_value * sin_value + in_value_2 * cos_value.
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>RoPE sin weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ seq, hidden state ]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[2]</Name>
                <Description>
                    <Content>RoPE cos weights</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>2D</Rank>
                    <Text>[ seq, hidden state  ]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMADequantize</Name>
            <Description>
                <Content>
                    LLaMA Dequantize
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>


        <OpDef>
            <Name>LLaMADequantizeAdd</Name>
            <Description>
                <Content>
                    LLaMA Dequantize and Add
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>input bias</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMAQuantize</Name>
            <Description>
                <Content>
                    LLaMA Quantize
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>scale</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
                <Default>N-1</Default>
            </Parameter>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>CausalMask</Name>
            <Description>
                <Content>
                    LLaMA CausalMask
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>input activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>
            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMAMul</Name>
            <Description>
                <Content>
                    LLaMA element-wise mul
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>X </Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>Y</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>


            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>LLaMAAdd</Name>
            <Description>
                <Content>
                    LLaMA element-wise add
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>X </Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>Y</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>output activation</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>


            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>KVCache</Name>
            <Description>
                <Content>
                    Decoder KVCache
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content> new KV activation output </Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>seq_pos</Name>
                <Description>
                    <Content>current output sequence position</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[1]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>New KVCache </Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>hidden_dim</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Parameter>


            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>PDKVCacheUpdate</Name>
            <Description>
                <Content>
                    PD-fusion KV cache update (in-place write into pre-allocated caches)
                </Content>
            </Description>

            <Input>
                <Name>dep</Name>
                <Description>
                    <Content>dependency tensor (unused, used to enforce scheduling)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in_k_cache</Name>
                <Description>
                    <Content>KV-cache key input (for shape)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>in_v_cache</Name>
                <Description>
                    <Content>KV-cache value input (for shape)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>present_k</Name>
                <Description>
                    <Content>new key states</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>present_v</Name>
                <Description>
                    <Content>new value states</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>n_past</Name>
                <Description>
                    <Content>write position (past tokens)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>

            <Input>
                <Name>src_offset</Name>
                <Description>
                    <Content>source offset in present KV</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>

            <Input>
                <Name>n_update</Name>
                <Description>
                    <Content>number of tokens to copy</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>

            <Input>
                <Name>enable</Name>
                <Description>
                    <Content>enable flag (0/1)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>

            <Output>
                <Name>out_k_cache</Name>
                <Description>
                    <Content>updated key cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Output>
                <Name>out_v_cache</Name>
                <Description>
                    <Content>updated value cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>FusedPDAttention</Name>
            <Description>
                <Content>
                    PD-fusion attention (prefill+decode in one op). v0 is a correctness-first reference kernel.
                </Content>
            </Description>

            <Input>
                <Name>q</Name>
                <Description>
                    <Content>query states</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>k_curr</Name>
                <Description>
                    <Content>current key states</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>v_curr</Name>
                <Description>
                    <Content>current value states</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>past_k_prefill</Name>
                <Description>
                    <Content>prefill-slot past key cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>past_v_prefill</Name>
                <Description>
                    <Content>prefill-slot past value cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>past_k_decode</Name>
                <Description>
                    <Content>decode-slot past key cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>past_v_decode</Name>
                <Description>
                    <Content>decode-slot past value cache</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>attention_mask</Name>
                <Description>
                    <Content>attention mask (same convention as KVCacheManager)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>fusion_ctrl</Name>
                <Description>
                    <Content>fusion control tensor [6]</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[6]</Text>
                </Shape>
            </Input>

            <Input>
                <Name>q_scale</Name>
                <Description>
                    <Content>Q dequant scale (float32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>q_zp</Name>
                <Description>
                    <Content>Q zero-point (int32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>k_scale</Name>
                <Description>
                    <Content>K dequant scale (float32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>k_zp</Name>
                <Description>
                    <Content>K zero-point (int32 scalar, typically 128 for uint8 sym)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>v_scale</Name>
                <Description>
                    <Content>V dequant scale (float32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>v_zp</Name>
                <Description>
                    <Content>V zero-point (int32 scalar, typically 128 for uint8 sym)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>out_scale</Name>
                <Description>
                    <Content>Output quant scale (float32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>
            <Input>
                <Name>out_zp</Name>
                <Description>
                    <Content>Output zero-point (int32 scalar)</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Input>

            <Output>
                <Name>out_attn</Name>
                <Description>
                    <Content>attention output</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

        <OpDef>
            <Name>WNop</Name>
            <Description>
                <Content>
                    CPU NPU Sync waiting op
                </Content>
            </Description>

            <Input>
                <Name>in[0]</Name>
                <Description>
                    <Content>sync input</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Layout>NHWC</Layout>
                    <Text>[1]</Text>
                </Shape>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Description>
                    <Content>sync input var</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[N, C, H , W]</Text>
                </Shape>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Description>
                    <Content>sync output</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>4D</Rank>
                    <Text> [N, C, H , W] </Text>
                </Shape>
            </Output>

            <Output>
                <Name>sync_var</Name>
                <Description>
                    <Content>sync singnal variable</Content>
                </Description>
                <Mandatory>true</Mandatory>
                <Datatype>BACKEND_SPECIFIC</Datatype>
                <Shape>
                    <Rank>1D</Rank>
                    <Text>[1]</Text>
                </Shape>
            </Output>

            <Parameter>
                <Name>sync_type</Name>
                <Mandatory>true</Mandatory>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
                <Shape>
                    <Rank>SCALAR</Rank>
                </Shape>
            </Parameter>


            <!--This Op is implemented on these Backends-->
            <SupportedBackend>HTP</SupportedBackend>
        </OpDef>

    </OpDefList>

    <SupplementalOpDefList Backend="HTP">
        <SupportedOps>
            <OpName>LLaMASuperSiLU</OpName>
            <OpName>SiLU</OpName>
            <OpName>RMSNorm</OpName>
            <OpName>RoPE</OpName>
            <OpName>IRoPE</OpName>
            <OpName>PDKVCacheUpdate</OpName>
            <OpName>FusedPDAttention</OpName>
            <OpName>LLaMAQuantize</OpName>
            <OpName>LLaMAMul</OpName>
            <OpName>LLaMAAdd</OpName>
            <OpName>LLaMAReLU</OpName>
            <OpName>CausalMask</OpName>
        </SupportedOps>

        <!--SiLU-->
        <SupplementalOpDef>
            <Name>SiLU</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>


            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--SiLU-->
        <SupplementalOpDef>
            <Name>LLaMASuperSiLU</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>



            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--LLaMAReLU-->
        <SupplementalOpDef>
            <Name>LLaMAReLU</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>


            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--LLaMALinear-->
        <SupplementalOpDef>
            <Name>LLaMALinear</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>in[2]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Output>

        </SupplementalOpDef>

        <!--Attention-->
        <SupplementalOpDef>
            <Name>Attention</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>
            <Input>
                <Name>in[2]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>in[3]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>in[4]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_16</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--FusedPDAttention-->
        <SupplementalOpDef>
            <Name>FusedPDAttention</Name>

            <Input>
                <Name>q</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_16</Datatype>
            </Input>
            <Input>
                <Name>k_curr</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>v_curr</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>past_k_prefill</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>past_v_prefill</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>past_k_decode</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>past_v_decode</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
            </Input>
            <Input>
                <Name>attention_mask</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_16</Datatype>
            </Input>
            <Input>
                <Name>fusion_ctrl</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
            </Input>
            <Input>
                <Name>q_scale</Name>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>q_zp</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
            </Input>
            <Input>
                <Name>k_scale</Name>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>k_zp</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
            </Input>
            <Input>
                <Name>v_scale</Name>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>v_zp</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
            </Input>
            <Input>
                <Name>out_scale</Name>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>out_zp</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
            </Input>

            <Output>
                <Name>out_attn</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_16</Datatype>
            </Output>
        </SupplementalOpDef>


        <!--QLayerNorm-->
        <SupplementalOpDef>
            <Name>QLayerNorm</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>weights</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>bias</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>


        <!--RMSNorm-->
        <SupplementalOpDef>
            <Name>RMSNorm</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>weights</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--RoPE-->
        <SupplementalOpDef>
            <Name>RoPE</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>sin</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>cos</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

             <Input>
                <Name>h_cnt</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

         <!--IRoPE-->
        <SupplementalOpDef>
            <Name>IRoPE</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>sin</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>

            <Input>
                <Name>cos</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
            </Input>

             <Input>
                <Name>h_cnt</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

         <!--LLaMAQuantize-->
        <SupplementalOpDef>
            <Name>LLaMAQuantize</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_16</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--LLaMADequantize-->
        <SupplementalOpDef>
            <Name>LLaMADequantize</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_16</Datatype>
            </Input>
            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--LLaMADequantize-->
        <SupplementalOpDef>
            <Name>LLaMADequantizeAdd</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_16</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--CausalMask-->
        <SupplementalOpDef>
            <Name>CausalMask</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--LLaMAMul-->
        <SupplementalOpDef>
            <Name>LLaMAMul</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>


        <!--LLaMAAdd-->
        <SupplementalOpDef>
            <Name>LLaMAAdd</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--KVCache-->
        <SupplementalOpDef>
            <Name>KVCache</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>seq_pos</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--PDKVCacheUpdate-->
        <SupplementalOpDef>
            <Name>PDKVCacheUpdate</Name>

            <Input>
                <Name>dep</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>in_k_cache</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>in_v_cache</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>present_k</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>present_v</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>

            <Input>
                <Name>n_past</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Input>
                <Name>src_offset</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Input>
                <Name>n_update</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Input>
                <Name>enable</Name>
                <Datatype>QNN_DATATYPE_INT_32</Datatype>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Output>
                <Name>out_k_cache</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>

            <Output>
                <Name>out_v_cache</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_SFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>
        </SupplementalOpDef>

        <!--WNop-->
        <SupplementalOpDef>
            <Name>WNop</Name>

            <Input>
                <Name>in[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Input>
            <Input>
                <Name>in[1]</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Input>

            <Output>
                <Name>out[0]</Name>
                <Datatype>QNN_DATATYPE_UFIXED_POINT_8</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_16</Datatype>
                <Datatype>QNN_DATATYPE_FLOAT_32</Datatype>
            </Output>

            <Output>
                <Name>sync_var</Name>
                <Datatype>QNN_DATATYPE_UINT_32</Datatype>
            </Output>

        </SupplementalOpDef>


    </SupplementalOpDefList>

</OpDefCollection>
